{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "We will explore this dataset: https://archive.ics.uci.edu/ml/datasets/EEG+Eye+State#\n",
    "\n",
    "> All data is from one continuous EEG measurement with the Emotiv EEG Neuroheadset. The duration of the measurement was 117 seconds. The eye state was detected via a camera during the EEG measurement and added later manually to the file after analysing the video frames. '1' indicates the eye-closed and '0' the eye-open state. All values are in chronological order with the first measured value at the top of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-05 10:24:40.876486: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-05 10:24:40.876525: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "data_dir = \"../../data/raw\"\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00264/EEG%20Eye%20State.arff\"\n",
    "datapath = tf.keras.utils.get_file(\n",
    "        \"eeg\", origin=url, untar=False, cache_dir=data_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load the arff file with scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "data = arff.loadarff(datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../data/raw/datasets/eeg'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is a tuple of a description and observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, tuple)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data), type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset: EEG_DATA\n",
       "\tAF3's type is numeric\n",
       "\tF7's type is numeric\n",
       "\tF3's type is numeric\n",
       "\tFC5's type is numeric\n",
       "\tT7's type is numeric\n",
       "\tP7's type is numeric\n",
       "\tO1's type is numeric\n",
       "\tO2's type is numeric\n",
       "\tP8's type is numeric\n",
       "\tT8's type is numeric\n",
       "\tFC6's type is numeric\n",
       "\tF4's type is numeric\n",
       "\tF8's type is numeric\n",
       "\tAF4's type is numeric\n",
       "\teyeDetection's type is nominal, range is ('0', '1')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are about 15k observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4329.2300, 4009.2300, 4289.2300, 4148.2100, 4350.2598, 4586.1499,\n",
       "        4096.9199, 4641.0298, 4222.0498, 4238.4600, 4211.2798, 4280.5098,\n",
       "        4635.8999, 4393.8501])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = data[0][0]\n",
    "observation = []\n",
    "obs = []\n",
    "for index, i in enumerate(line):\n",
    "    if index != 14:\n",
    "        observation.append(i)\n",
    "observation = torch.Tensor(observation)\n",
    "observation\n",
    "\n",
    "torch.stack(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observations are tuples of floats and a byte as label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([682, 14])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_label = int(data[0][0][14])\n",
    "label = first_label\n",
    "chunck = []\n",
    "chuncks = []\n",
    "for line in data[0]:\n",
    "    if int(line[14]) == label:\n",
    "        observation = []\n",
    "        for index, i in enumerate(line):\n",
    "            if index != 14:\n",
    "                observation.append(i)\n",
    "        observation = torch.Tensor(observation)\n",
    "        chunck.append(observation)\n",
    "    else:\n",
    "        chunck_tuple = (label, torch.stack(chunck))\n",
    "        chuncks.append(chunck_tuple)\n",
    "        label = int(line[14])\n",
    "        chunck = []\n",
    "        observation = []\n",
    "        for index, i in enumerate(line):\n",
    "            if index != 14:\n",
    "                observation.append(i)\n",
    "        observation = torch.Tensor(observation)\n",
    "chunck_tuple = (label, torch.stack(chunck))\n",
    "chuncks.append(chunck_tuple)\n",
    "\n",
    "\n",
    "# for chunck in chuncks:\n",
    "#     print(chunck[0], len(chunck[1]))\n",
    "\n",
    "chuncks[1][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'0'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = []\n",
    "for x in data[0]:\n",
    "    labels.append(int(x[14]))\n",
    "\n",
    "data[0][0][14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4487983978638184"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(labels).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 45% of the data has closed eyes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercises 1\n",
    "\n",
    "- create a get_eeg function that downloads the data to a given path\n",
    "- build a Dataset that yields a ($X, y$) tuple of tensors. $X$ should be sequential in time. Remember: a dataset should implement `__get_item__` and `__len__`.\n",
    "- note that you could model this as both a classification task, but also as a sequence-to-sequence task! For this excercise, make it a classification task with consecutive 0s or 1s only.\n",
    "- Note that, for a training task, a seq2seq model will probably be more realistic. However, the classification is a nice excercise because it is harder to set up.\n",
    "- figure out what the length distribution is of your dataset: how many timestamps do you have for every consecutive sequence of 0s and 1s? On average, median, min, max?\n",
    "- create a dataloader that yields timeseries with (batch, sequence_lenght). You can implement: windowed, padded and batched.\n",
    "    1. yielding a windowed item should be the easy level\n",
    "    2. yielding windowed and padded is medium level \n",
    "    3. yielding windowed, padded and batched is expert level, because the windowing will cause the timeseries to have different sizes. You will need to buffer before you can yield a batch.\n",
    "\n",
    "1. Upload this to github. \n",
    "2. Put your dev notebooks in a seperate folder\n",
    "3. Put all your functions in the src folder\n",
    "4. Use a formater & linter\n",
    "5. Add a single notebook, that sources the src folder. Indicate which level you got (1, 2 or 3)\n",
    "6. and that shows your dataloader works:\n",
    "    - it should not give errors because it runs out of data! Either let is stop by itself, or run forever.\n",
    "    - batchsize should be consistent (in case 1 and 2, batchsize is 1)\n",
    "    - sequence length is allowed to vary\n",
    "\n",
    "The first excercise is ex1, this one is ex2. You will get $max(ex1, average(ex1, ex2))$ as a final remark.\n",
    "Level 3 can get you an 11, because it exceeds expectation.\n",
    "\n",
    "# Excercise 2\n",
    "- build a Dataset that yields sequences of X, y. This time, y is a sequence and can contain both 0s and 1s\n",
    "- create a Dataloader with this\n",
    "- Test appropriate architectures (RNN, Attention)\n",
    "- for the loss, note that you will need a BCELoss instead of a CrossEntroyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 7,\n",
       " 22,\n",
       " 20,\n",
       " 21,\n",
       " 10,\n",
       " 16,\n",
       " 2,\n",
       " 5,\n",
       " 9,\n",
       " 15,\n",
       " 8,\n",
       " 0,\n",
       " 18,\n",
       " 11,\n",
       " 17,\n",
       " 19,\n",
       " 14,\n",
       " 12,\n",
       " 13]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "list_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-05 13:48:38.776 | INFO     | __main__:get_eeg:26 - Data is downloaded to ../../data/raw/datasets/eeg.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([51, 14])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import random\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Callable, Dict, Iterator, List, Optional, Sequence, Tuple, Union\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from loguru import logger\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from scipy.io import arff\n",
    "\n",
    "Tensor = torch.Tensor\n",
    "\n",
    "def get_eeg(data_dir: Path = \"../../data/raw\") -> Path:\n",
    "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00264/EEG%20Eye%20State.arff\"\n",
    "    datapath = tf.keras.utils.get_file(\n",
    "        \"eeg\", origin=url, untar=False, cache_dir=data_dir\n",
    "    )\n",
    "    datapath = Path(datapath)\n",
    "    logger.info(f\"Data is downloaded to {datapath}.\")\n",
    "    return datapath\n",
    "\n",
    "class BaseDataset:\n",
    "    def __init__(self, datapath: Path):\n",
    "        self.path = datapath\n",
    "        self.data = self.process_data()\n",
    "        \n",
    "    def process_data(self) -> None:\n",
    "        data = arff.loadarff(self.path)\n",
    "        first_label = int(data[0][0][14])\n",
    "        label = first_label\n",
    "        chunck = []\n",
    "        chuncks = []\n",
    "        for line in data[0]:\n",
    "            if int(line[14]) == label:\n",
    "                observation = []\n",
    "                for index, i in enumerate(line):\n",
    "                    if index != 14:\n",
    "                        observation.append(i)\n",
    "                observation = torch.Tensor(observation)\n",
    "                chunck.append(observation)\n",
    "            else:\n",
    "                chunck_tuple = (label, torch.stack(chunck))\n",
    "                chuncks.append(chunck_tuple)\n",
    "                label = int(line[14])\n",
    "                chunck = []\n",
    "                observation = []\n",
    "                for index, i in enumerate(line):\n",
    "                    if index != 14:\n",
    "                        observation.append(i)\n",
    "                observation = torch.Tensor(observation)\n",
    "        chunck_tuple = (label, torch.stack(chunck))\n",
    "        chuncks.append(chunck_tuple)\n",
    "        return chuncks\n",
    "\n",
    "    def __getitem__(self):\n",
    "        lenght = self.__len__()\n",
    "        list_index = list(range(0,lenght))\n",
    "        random.shuffle(list_index)\n",
    "        return(self.data[list_index[0]])\n",
    "\n",
    "    def __len__(self):\n",
    "        length = len(self.data)\n",
    "        return length\n",
    "\n",
    "dataloader = BaseDataset(datapath = get_eeg())\n",
    "dataloader.__getitem__()[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-05 13:33:33.336 | INFO     | __main__:get_eeg:26 - Data is downloaded to ../../data/raw/datasets/eeg.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, tensor([[4256.9199, 4010.7700, 4261.0298,  ..., 4271.2798, 4540.0000,\n",
      "         4293.3301],\n",
      "        [4259.4902, 4011.7900, 4261.0298,  ..., 4275.3799, 4543.5898,\n",
      "         4298.4600],\n",
      "        [4257.9502, 4020.5100, 4255.8999,  ..., 4275.3799, 4548.2100,\n",
      "         4307.1802],\n",
      "        ...,\n",
      "        [4402.5601, 4005.1299, 4287.1802,  ..., 4323.0801, 4736.9199,\n",
      "         4496.9199],\n",
      "        [4386.6699, 3998.4600, 4276.4102,  ..., 4313.8501, 4727.6899,\n",
      "         4486.1499],\n",
      "        [4378.9702, 3990.7700, 4271.7900,  ..., 4314.8701, 4734.3599,\n",
      "         4485.1299]]))\n"
     ]
    }
   ],
   "source": [
    "def window(x: Tensor, n_time: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    Generates and index that can be used to window a timeseries.\n",
    "    E.g. the single series [0, 1, 2, 3, 4, 5] can be windowed into 4 timeseries with\n",
    "    length 3 like this:\n",
    "\n",
    "    [0, 1, 2]\n",
    "    [1, 2, 3]\n",
    "    [2, 3, 4]\n",
    "    [3, 4, 5]\n",
    "\n",
    "    We now can feed 4 different timeseries into the model, instead of 1, all\n",
    "    with the same length.\n",
    "    \"\"\"\n",
    "    n_window = len(x) - n_time + 1\n",
    "    time = torch.arange(0, n_time).reshape(1, -1)\n",
    "    window = torch.arange(0, n_window).reshape(-1, 1)\n",
    "    idx = time + window\n",
    "    return idx\n",
    "\n",
    "\n",
    "class BaseDataIterator:\n",
    "    def __init__(self, dataset: BaseDataset, batchsize: int) -> None:\n",
    "        self.dataset = dataset\n",
    "        self.batchsize = batchsize\n",
    "        self.item = self.dataset.__getitem__()\n",
    "        print(self.item)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return int(len(self.dataset) / self.batchsize)\n",
    "\n",
    "    def __iter__(self) -> BaseDataIterator:\n",
    "        self.index = 0\n",
    "        self.index_list = torch.randperm(len(self.dataset))\n",
    "        return self\n",
    "\n",
    "    def batchloop(self) -> Tuple[List, List]:\n",
    "        X = []  # noqa N806\n",
    "        Y = []  # noqa N806\n",
    "        for _ in range(self.batchsize):\n",
    "            x, y = self.dataset[int(self.index_list[self.index])]\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "            self.index += 1\n",
    "        return X, Y\n",
    "\n",
    "    def __next__(self) -> Tuple[Tensor, Tensor]:\n",
    "        if self.index <= (len(self.dataset) - self.batchsize):\n",
    "            X, Y = self.batchloop()  # noqa N806\n",
    "            return torch.tensor(X), torch.tensor(Y)\n",
    "        else:\n",
    "            raise StopIteration\n",
    "\n",
    "class PaddedDatagenerator(BaseDataIterator):\n",
    "    \"\"\"Iterator with additional padding of X\n",
    "\n",
    "    Args:\n",
    "        BaseDataIterator (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset: BaseDataset, batchsize: int) -> None:\n",
    "        super().__init__(dataset, batchsize)\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.index <= (len(self.dataset) - self.batchsize):\n",
    "            X, Y = self.batchloop()  # noqa N806\n",
    "            X_ = pad_sequence(X, batch_first=True, padding_value=0)  # noqa N806\n",
    "            return X_, torch.tensor(Y)\n",
    "        else:\n",
    "            raise StopIteration\n",
    "\n",
    "dataset = BaseDataset(datapath = get_eeg())\n",
    "loader = BaseDataIterator(dataset = dataset, batchsize=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BaseDataIterator' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/mladmin/code/ProjectML/projectml/notebooks/exercise/06_excercise.ipynb Cell 21'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e696e6b654d4c227d/home/mladmin/code/ProjectML/projectml/notebooks/exercise/06_excercise.ipynb#ch0000031vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mnext\u001b[39;49m(loader)\n",
      "\u001b[1;32m/home/mladmin/code/ProjectML/projectml/notebooks/exercise/06_excercise.ipynb Cell 20'\u001b[0m in \u001b[0;36mBaseDataIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e696e6b654d4c227d/home/mladmin/code/ProjectML/projectml/notebooks/exercise/06_excercise.ipynb#ch0000029vscode-remote?line=44'>45</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Tensor, Tensor]:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e696e6b654d4c227d/home/mladmin/code/ProjectML/projectml/notebooks/exercise/06_excercise.ipynb#ch0000029vscode-remote?line=45'>46</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m (\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset) \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatchsize):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e696e6b654d4c227d/home/mladmin/code/ProjectML/projectml/notebooks/exercise/06_excercise.ipynb#ch0000029vscode-remote?line=46'>47</a>\u001b[0m         X, Y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatchloop()  \u001b[39m# noqa N806\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e696e6b654d4c227d/home/mladmin/code/ProjectML/projectml/notebooks/exercise/06_excercise.ipynb#ch0000029vscode-remote?line=47'>48</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mtensor(X), torch\u001b[39m.\u001b[39mtensor(Y)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BaseDataIterator' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "next(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PaddedDatagenerator(BaseDataIterator):\n",
    "    \"\"\"Iterator with additional padding of X\n",
    "\n",
    "    Args:\n",
    "        BaseDataIterator (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset: BaseDataset, batchsize: int) -> None:\n",
    "        super().__init__(dataset, batchsize)\n",
    "\n",
    "    def __next__(self) -> Tuple[Tensor, Tensor]:\n",
    "        if self.index <= (len(self.dataset) - self.batchsize):\n",
    "            X, Y = self.batchloop()  # noqa N806\n",
    "            X_ = pad_sequence(X, batch_first=True, padding_value=0)  # noqa N806\n",
    "            return X_, torch.tensor(Y)\n",
    "        else:\n",
    "            raise StopIteration\n",
    "\n",
    "\n",
    "class BaseDatastreamer:\n",
    "    \"\"\"This datastreamer will never stop\n",
    "    The dataset should have a:\n",
    "        __len__ method\n",
    "        __getitem__ method\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: BaseDataset,\n",
    "        batchsize: int,\n",
    "        preprocessor: Optional[Callable] = None,\n",
    "    ) -> None:\n",
    "        self.dataset = dataset\n",
    "        self.batchsize = batchsize\n",
    "        self.preprocessor = preprocessor\n",
    "        self.size = len(self.dataset)\n",
    "        self.reset_index()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return int(len(self.dataset) / self.batchsize)\n",
    "\n",
    "    def reset_index(self) -> None:\n",
    "        self.index_list = np.random.permutation(self.size)\n",
    "        self.index = 0\n",
    "\n",
    "    def batchloop(self) -> Sequence[Tuple]:\n",
    "        batch = []\n",
    "        for _ in range(self.batchsize):\n",
    "            x, y = self.dataset[int(self.index_list[self.index])]\n",
    "            batch.append((x, y))\n",
    "            self.index += 1\n",
    "        return batch\n",
    "\n",
    "    def stream(self) -> Iterator:\n",
    "        while True:\n",
    "            if self.index > (self.size - self.batchsize):\n",
    "                self.reset_index()\n",
    "            batch = self.batchloop()\n",
    "            if self.preprocessor is not None:\n",
    "                X, Y = self.preprocessor(batch)  # noqa N806\n",
    "            else:\n",
    "                X, Y = zip(*batch)  # noqa N806\n",
    "            yield X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import random\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Callable, Dict, Iterator, List, Optional, Sequence, Tuple, Union\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from loguru import logger\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from src.data import data_tools\n",
    "from src.data.data_tools import PaddedDatagenerator, TSDataset\n",
    "\n",
    "def window(x: Tensor, n_time: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    Generates and index that can be used to window a timeseries.\n",
    "    E.g. the single series [0, 1, 2, 3, 4, 5] can be windowed into 4 timeseries with\n",
    "    length 3 like this:\n",
    "\n",
    "    [0, 1, 2]\n",
    "    [1, 2, 3]\n",
    "    [2, 3, 4]\n",
    "    [3, 4, 5]\n",
    "\n",
    "    We now can feed 4 different timeseries into the model, instead of 1, all\n",
    "    with the same length.\n",
    "    \"\"\"\n",
    "    n_window = len(x) - n_time + 1\n",
    "    time = torch.arange(0, n_time).reshape(1, -1)\n",
    "    window = torch.arange(0, n_window).reshape(-1, 1)\n",
    "    idx = time + window\n",
    "    return idx\n",
    "\n",
    "\n",
    "\n",
    "def get_eeg(data_dir: Path = \"../../data/raw\") -> Path:\n",
    "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00264/EEG%20Eye%20State.arff\"\n",
    "    datapath = tf.keras.utils.get_file(\n",
    "        \"eeg\", origin=url, untar=False, cache_dir=data_dir\n",
    "    )\n",
    "    datapath = Path(datapath)\n",
    "    logger.info(f\"Data is downloaded to {datapath}.\")\n",
    "    return datapath\n",
    "\n",
    "class Dataloader:\n",
    "    def __init__(self, datapath: Path):\n",
    "        self.path = datapath\n",
    "        self.dataset = []\n",
    "        self.process_data()\n",
    "\n",
    "    def __get_item__():\n",
    "        pass\n",
    "\n",
    "    def __len__():\n",
    "        pass\n",
    "\n",
    "    def process_data(self) -> None:\n",
    "        file = self.path\n",
    "        x_ = np.genfromtxt(file)[:, 3:]\n",
    "        x = torch.tensor(x_).type(torch.float32)\n",
    "        y = torch.tensor(int(file.parent.name) - 1)\n",
    "        self.dataset.append((x, y))\n",
    "\n",
    "class BaseDataset:\n",
    "    def __init__(self, paths: List[Path]) -> None:\n",
    "        self.paths = paths\n",
    "        random.shuffle(self.paths)\n",
    "        self.dataset = []\n",
    "        self.process_data()\n",
    "\n",
    "    def process_data(self) -> None:\n",
    "        for file in tqdm(self.paths):\n",
    "            x_ = np.genfromtxt(file)[:, 3:]\n",
    "            x = torch.tensor(x_).type(torch.float32)\n",
    "            y = torch.tensor(int(file.parent.name) - 1)\n",
    "            self.dataset.append((x, y))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Tensor, int]:\n",
    "        return self.dataset[idx]\n",
    "\n",
    "class BaseDataIterator:\n",
    "    def __init__(self, dataset: BaseDataset, batchsize: int):\n",
    "        self.dataset = dataset\n",
    "        self.batchsize = batchsize\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # the lenght is the amount of batches\n",
    "        return int(len(self.dataset) / self.batchsize)\n",
    "\n",
    "    def __iter__(self) -> BaseDataIterator:\n",
    "        # initialize index\n",
    "        self.index = 0\n",
    "        self.index_list = torch.randperm(len(self.dataset))\n",
    "        return self\n",
    "    \n",
    "    def batchloop(self) -> Tuple[Tensor, Tensor]:\n",
    "        X = []  # noqa N806\n",
    "        Y = []  # noqa N806\n",
    "        # fill the batch\n",
    "        for _ in range(self.batchsize):\n",
    "            x, y = self.dataset[int(self.index_list[self.index])]\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "            self.index += 1\n",
    "        return X, Y\n",
    "\n",
    "    def __next__(self) -> Tuple[Tensor, Tensor]:\n",
    "        if self.index <= (len(self.dataset) - self.batchsize):\n",
    "            X, Y = self.batchloop()\n",
    "            return X, Y\n",
    "        else:\n",
    "            raise StopIteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BCELoss example\n",
    "In this example, which input would you prefer for the given target?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input1 = torch.tensor([0.1, 0.1, 0.7, 0.9])\n",
    "input2 = torch.tensor([0.1, 0.3, 0.6, 0.7])\n",
    "target = torch.tensor([0., 0., 1., 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, which loss should you pick? CrossEntropyLoss won't work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension out of range (expected to be in range of [-1, 0], but got 1)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "try:\n",
    "    loss(input1, target)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need BCELoss for this.\n",
    "Binary cross entropy loss works like this:\n",
    "$$X = {x_i, \\dots, x_n}$$\n",
    "\n",
    "$$l_i =-(y_i \\cdot log(x_i) + (1-y_i) \\cdot log(1-x_i))$$\n",
    "$$BCELoss = mean(l)$$\n",
    "\n",
    "Note that the labels are assumed to be either 0 or 1 (hence, the binary part).\n",
    "If a label is 0, only the second part is relevent. If the label is 1, only the first part is relevant. the default reduction is \"mean\":\n",
    "\n",
    "$$\n",
    "BCEloss = \n",
    "\\begin{cases}\n",
    "mean(-log(1 - x_i)) & \\text{if\\,} y = 0\\\\\n",
    "mean(-log(x_i)) & \\text{if\\,} y = 1\n",
    "\\end{cases}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can see this works nice for a sequence of 0s and 1s.\n",
    "You can see that input1 is preferred, because it is more certain of the cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1682), tensor(0.3324))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.BCELoss()\n",
    "loss(input1, target), loss(input2, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or a more generic example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8594)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.Sigmoid() # make sure outputs are between 0 and 1\n",
    "X = torch.randn(100) # generate 100 random inputs\n",
    "yhat = m(X) # our dummy model\n",
    "\n",
    "p = torch.ones_like(yhat) / 2\n",
    "y = torch.bernoulli(p) # we create a random label sequence of 0s and 1s\n",
    "loss(yhat, y)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16b8f312320cd240106b9ea4d318428341e8727b3c7d5fc1f73cfe4a3d9868ce"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('deep-learning-E14Cnx23-py3.9': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
